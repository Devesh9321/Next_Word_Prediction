{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd11d7c6-f9ba-40dd-8173-992277d6abb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3438e379-ddbd-4c36-a741-8046734c6fdc",
   "metadata": {},
   "source": [
    "### Importing the Libarires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "500e5914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import collections\n",
    "import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from keras.utils import to_categorical\n",
    "# import scipy\n",
    "# from scipy import spatial\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "tokenizer = ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60cfc261-b400-4947-8e9c-0ef1ad22a611",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"./Dataset./news_articles.csv\", encoding='ISO-8859-1')\n",
    "\n",
    "df.isnull()\n",
    "df.isnull().sum().sum()\n",
    "df.dropna(inplace=True)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6d18af80-b197-419c-a0f5-43a10f2fdfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2045 entries, 0 to 2045\n",
      "Data columns (total 12 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   author                   2045 non-null   object \n",
      " 1   published                2045 non-null   object \n",
      " 2   title                    2045 non-null   object \n",
      " 3   text                     2045 non-null   object \n",
      " 4   language                 2045 non-null   object \n",
      " 5   site_url                 2045 non-null   object \n",
      " 6   main_img_url             2045 non-null   object \n",
      " 7   type                     2045 non-null   object \n",
      " 8   label                    2045 non-null   object \n",
      " 9   title_without_stopwords  2045 non-null   object \n",
      " 10  text_without_stopwords   2045 non-null   object \n",
      " 11  hasImage                 2045 non-null   float64\n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 207.7+ KB\n",
      "None\n",
      "['print they should pay all the back all the money plus interest the entire family and everyone who came in with them need to be deported asap why did it take two years to bust them \\nhere we go again another group stealing from the government and taxpayers a group of somalis stole over four million in government benefits over just  months \\nweve reported on numerous cases like this one where the muslim refugeesimmigrants commit fraud by scamming our systemits way out of control more related', 'why did attorney general loretta lynch plead the fifth barracuda brigade  print the administration is blocking congressional probe into cash payments to iran of course she needs to plead the th she either cant recall refuses to answer or just plain deflects the question straight up corruption at its finest \\npercentfedupcom  talk about covering your ass loretta lynch did just that when she plead the fifth to avoid incriminating herself over payments to irancorrupt to the core attorney general loretta lynch is declining to comply with an investigation by leading members of congress about the obama administrations secret efforts to send iran  billion in cash earlier this year prompting accusations that lynch has pleaded the fifth amendment to avoid incriminating herself over these payments according to lawmakers and communications exclusively obtained by the washington free beacon \\nsen marco rubio r fla and rep mike pompeo r kan initially presented lynch in october with a series of questions about how the cash payment to iran was approved and delivered \\nin an oct  response assistant attorney general peter kadzik responded on lynchs behalf refusing to answer the questions and informing the lawmakers that they are barred from publicly disclosing any details about the cash payment which was bound up in a ransom deal aimed at freeing several american hostages from iran \\nthe response from the attorney generals office is unacceptable and provides evidence that lynch has chosen to essentially plead the fifth and refuse to respond to inquiries regarding herrole in providing cash to the worlds foremost state sponsor of terrorism rubio and pompeo wrote on friday in a followup letter to lynch more related', 'red state  \\nfox news sunday reported this morning that anthony weiner is cooperating with the fbi which has reopened yes lefties reopened the investigation into hillary clintons classified emails watch as chris wallace reports the breaking news during the panel segment near the end of the show \\nand the news is breaking while were on the air our colleague bret baier has just sent us an email saying he has two sources who say that anthony weiner who also had coownership of that laptop with his estranged wife huma abedin is cooperating with the fbi investigation had given them the laptop so therefore they didnt need a warrant to get in to see the contents of said laptop pretty interesting development \\ntargets of federal investigations will often cooperate hoping that they will get consideration from a judge at sentencing given weiners wellknown penchant for lying its hard to believe that a prosecutor would give weiner a deal based on an agreement to testify unless his testimony were very strongly corroborated by hard evidence but cooperation can take many forms  and as wallace indicated on this mornings show one of those forms could be signing a consent form to allow   the contents of devices that they could probably get a warrant for anyway well see if weiners cooperation extends beyond that more related']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# df['text'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "Text_data = df['text'].values\n",
    "\n",
    "list_data = Text_data.tolist()\n",
    "print(list_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "898b449f-e642-40d3-864f-92d85d96bd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print they should pay all the back all the money plus interest the entire family and everyone who ca\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TextData = ''.join(list_data)\n",
    "\n",
    "TextData = TextData.replace('\\n', '')\n",
    "TextData = TextData.lower()\n",
    "\n",
    "# Convert to lowercase and remove special characters\n",
    "TextData = re.sub(r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])', ' ', TextData)\n",
    "TextData = re.sub(r'[^a-zA-z0-9\\s]', '', ''.join(TextData))\n",
    "\n",
    "# Lemmatize words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "TextData = ' '.join([lemmatizer.lemmatize(word) for word in TextData.split()])\n",
    "\n",
    "print(TextData[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "73bf0da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Word Cound :  45923\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing\n",
    "tokens = tokenizer.tokenize(TextData)\n",
    "tokens = [token.strip() for token in tokens]\n",
    "\n",
    "# get the distinct words and sort it\n",
    "word_counts = collections.Counter(tokens)\n",
    "word_c = len(word_counts)\n",
    "print('Total Word Cound : ', word_c)\n",
    "\n",
    "distinct_words = [x[0] for x in word_counts.most_common()]\n",
    "distinct_words_sorted = list(sorted(distinct_words))\n",
    "# Generate indexing for all words\n",
    "word_index = {x: i for i, x in enumerate(distinct_words_sorted)}\n",
    "# decide on sentence length\n",
    "sentence_length = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a57eb6",
   "metadata": {},
   "source": [
    "## prepare the dataset of input to output pairs encoded as integers\n",
    "#### Generate the data for the model\n",
    "input = the input sentence to the model with index\n",
    "\n",
    "output = output of the model with index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "73053358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31587, 40587, 36766, 29734, 1168, 40416, 3273, 1168, 40416, 26115, 30691, 20617, 40416, 12966, 14226, 1630, 13543, 44714, 5854, 19855, 44986, 40441, 27062, 41018, 3719]]\n",
      "\n",
      "\n",
      "[10260]\n"
     ]
    }
   ],
   "source": [
    "InputData = []\n",
    "OutputData = []\n",
    "for i in range(0, word_c - sentence_length, 1):\n",
    "    X = tokens[i:i + sentence_length]\n",
    "    Y = tokens[i + sentence_length]\n",
    "    InputData.append([word_index[char] for char in X])\n",
    "    OutputData.append(word_index[Y])\n",
    "\n",
    "print(InputData[:1])\n",
    "print(\"\\n\")\n",
    "print(OutputData[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "97f9e5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length X : 45898 | length Y : 45898\n"
     ]
    }
   ],
   "source": [
    "# Generate X\n",
    "X = numpy.reshape(InputData, (len(InputData), sentence_length, 1))\n",
    "\n",
    "# One hot encode the output variable\n",
    "Y = to_categorical(OutputData)\n",
    "\n",
    "print(f'length X : {len(X)} | length Y : {len(Y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e4cefa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# define the checkpoint\n",
    "file_name_path = \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    file_name_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "25429e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "359/359 [==============================] - ETA: 0s - loss: 7.8245\n",
      "Epoch 1: loss improved from inf to 7.82449, saving model to weights-improvement-01-7.8245.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patil\\OneDrive\\Desktop\\Python Projects\\Text Generative AI\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359/359 [==============================] - 183s 492ms/step - loss: 7.8245\n",
      "Epoch 2/5\n",
      "359/359 [==============================] - ETA: 0s - loss: 7.2877\n",
      "Epoch 2: loss improved from 7.82449 to 7.28767, saving model to weights-improvement-02-7.2877.hdf5\n",
      "359/359 [==============================] - 153s 425ms/step - loss: 7.2877\n",
      "Epoch 3/5\n",
      "359/359 [==============================] - ETA: 0s - loss: 7.1463\n",
      "Epoch 3: loss improved from 7.28767 to 7.14630, saving model to weights-improvement-03-7.1463.hdf5\n",
      "359/359 [==============================] - 154s 429ms/step - loss: 7.1463\n",
      "Epoch 4/5\n",
      "359/359 [==============================] - ETA: 0s - loss: 7.0502\n",
      "Epoch 4: loss improved from 7.14630 to 7.05020, saving model to weights-improvement-04-7.0502.hdf5\n",
      "359/359 [==============================] - 148s 411ms/step - loss: 7.0502\n",
      "Epoch 5/5\n",
      "359/359 [==============================] - ETA: 0s - loss: 7.0113\n",
      "Epoch 5: loss improved from 7.05020 to 7.01130, saving model to weights-improvement-05-7.0113.hdf5\n",
      "359/359 [==============================] - 153s 427ms/step - loss: 7.0113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x24354e58070>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, Y, epochs=5, batch_size=128, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8ddf7f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "file_name = \"weights-improvement-05-7.0113.hdf5\"\n",
    "model.load_weights(file_name)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3407577f",
   "metadata": {},
   "source": [
    "## Predicting next word\n",
    "We will randomly generate a sequence of words and input to the model \n",
    "and see what it predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "63600e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20732, 6187, 29744, 41018, 20889, 28308, 8794, 36582, 27062, 41018, 30633, 40416, 40374, 36582, 12318, 5978, 33050, 33358, 41018, 1832, 28730, 21765, 30552, 9960, 40416]\n",
      "\n",
      "\n",
      "40416\n"
     ]
    }
   ],
   "source": [
    "# Generating random sequence\n",
    "# start = numpy.random.randint(0, len(InputData))\n",
    "start = 103\n",
    "input_sent = InputData[start]\n",
    "# input_sent = InputData[start]\n",
    "\n",
    "# Generate index of the next word of the email\n",
    "X = numpy.reshape(input_sent, (1, len(input_sent), 1))\n",
    "\n",
    "predict_word = model.predict(X, verbose=0)\n",
    "index = numpy.argmax(predict_word)\n",
    "\n",
    "print(input_sent)\n",
    "print(\"\\n\")\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f78d889b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['on', 'to', 'so', 'and', 'partnership', 'that', 'in', 'credit', 'halfjokingly', 'and', 'they', 'even', 'been', 'credit', 'evidence', 'comment', 'getting', 'of', 'and', 'thing', 'richard', 'again', 'attendance', 'democracy', 'even']\n",
      "\n",
      "\n",
      "even\n"
     ]
    }
   ],
   "source": [
    "# Convert these indexes back to words\n",
    "\n",
    "word_index_rev = dict((i, c) for i, c in enumerate(tokens))\n",
    "result = word_index_rev[index]\n",
    "sent_in = [word_index_rev[value] for value in input_sent]\n",
    "\n",
    "print(sent_in)\n",
    "print(\"\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ff802-12b8-4329-91fa-dfc150e02684",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define hyperparameters\n",
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "Model_Results = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dc9ea9",
   "metadata": {},
   "source": [
    "Creating Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e67774-0bea-4d23-a9b6-aa31936df0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word embeddings\n",
    "embeddings_index = {}\n",
    "with open('/content/drive/MyDrive/Fresh_Project/Asserts/glove.6B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Create embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
